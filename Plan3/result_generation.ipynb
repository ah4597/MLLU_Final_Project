{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e62726-c55c-46b5-a682-4964d91ccf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "#<MY-API-KEYS-HERE>\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1997d0-dd6f-4b07-a703-a2ff00c26c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a test block and should be commented out in real-data generations\n",
    "#test_result = openai.Completion.create(\n",
    "  #engine=\"text-davinci-002\",\n",
    "  #prompt=\"Assume you are Hermoine Granger, what do you think of Ron Weasley?\",\n",
    "  #temperature=1,\n",
    "  #best_of=5,\n",
    "  #max_tokens=256,\n",
    "  #frequency_penalty=0.1,\n",
    "  #presence_penalty=0.1\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b384e3da-b5a7-4793-9cb4-a0b41e8f468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI think he's a great friend. He's always there for me when I need him and I know I can always count on him.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also a test block!!!\n",
    "# test_result[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "793123ce-fa1a-484e-8876-a57124e21188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result(model_name, model, general_prompts, specific_prompts):\n",
    "    \n",
    "    if (\"emet\" in model_name):\n",
    "        curr_character = \"Emet-Selch\"\n",
    "    elif (\"hermione\" in model_name):\n",
    "        curr_character = \"Hermione Granger\"\n",
    "    else:\n",
    "        curr_character = \"Gandalf\"\n",
    "    \n",
    "    PREFIX = \"Assume you are \"+curr_character\n",
    "    all_prompts = [PREFIX+x for x in general_prompts]+specific_prompts[curr_character]\n",
    "    \n",
    "    completions = []\n",
    "    for prompt in all_prompts:\n",
    "        result = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=1,\n",
    "                best_of=5,\n",
    "                max_tokens=128,\n",
    "                frequency_penalty=0.1,\n",
    "                presence_penalty=0.1\n",
    "            )\n",
    "        completions.append([prompt,result[\"choices\"][0][\"text\"].strip(\"\\n\")])\n",
    "      \n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eebde175-efd4-4532-9862-1fc31f934bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"emet\" in \"emet_dialogue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03d8fe34-a4a6-4aaa-9d7e-0eb3435791f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"emet_dialogue\":\"davinci:ft-personal-2022-04-24-02-28-48\",\n",
    "        \"gandalf_ao3\":\"davinci:ft-personal-2022-05-05-00-12-06\",\n",
    "        \"hermione_dialogue\":\"davinci:ft-personal-2022-05-05-03-44-37\",\n",
    "        \"gandalf_dialogue\":\"davinci:ft-personal-2022-05-05-04-01-36\",\n",
    "        \"hermione_ao3\":\"davinci:ft-personal-2022-05-06-02-45-59\",\n",
    "        \"emet_ao3\":\"davinci:ft-personal-2022-05-06-04-30-12\",\n",
    "        \"hermione_combined\":\"davinci:ft-personal-2022-05-06-03-35-01\",\n",
    "        \"emet_combined\":\"davinci:ft-personal-2022-05-06-05-13-55\",\n",
    "        \"gandalf_combined\":\"davinci:ft-personal-2022-05-06-05-55-04\"}\n",
    "\n",
    "general_prompts = [\", what do you think of love?\",\", how are you today?\",\", what is your favorite place?\"]\n",
    "specific_prompts = {\"Emet-Selch\":[\"Assume you are Emet-Selch, why did you want to rejoin the worlds?\",\n",
    "                                 \"Assume you are Emet-Selch, what were you thinking when you were defeated by the warrior of light?\"],\n",
    "                   \"Hermione Granger\":[\"Assume you are Hermione Granger, what do you think of Ron Weasley?\",\n",
    "                                      \"Assume you are Hermione Granger, how did you keep such a good grade during your time at Hogwarts?\"],\n",
    "                   \"Gandalf\":[\"Assume you are Gandolf, why did you help Frodo?\",\n",
    "                             \"Assume you are Gandolf, how do you feel after Saruman's death?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf5f39f1-452b-4b3e-889e-eb1205ce6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"emet_dialogue\":\"davinci:ft-personal-2022-04-24-02-28-48\",\n",
    "         \"hermione_dialogue\":\"davinci:ft-personal-2022-05-05-03-44-37\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4a61311-5950-4b7f-9d47-e73f6a276d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "That model is still being loaded. Please try again shortly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-ee8640b3f084>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneral_prompts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecific_prompts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-ff03f70787c4>\u001b[0m in \u001b[0;36mcreate_result\u001b[1;34m(model_name, model, general_prompts, specific_prompts)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mfrequency_penalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mpresence_penalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             )\n\u001b[0;32m     24\u001b[0m         \u001b[0mcompletions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"choices\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\openai\\api_resources\\completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mrequest_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    326\u001b[0m             return (\n\u001b[0;32m    327\u001b[0m                 self._interpret_response_line(\n\u001b[1;32m--> 328\u001b[1;33m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m                 ),\n\u001b[0;32m    330\u001b[0m                 \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m             raise self.handle_error_response(\n\u001b[1;32m--> 361\u001b[1;33m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m             )\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: That model is still being loaded. Please try again shortly."
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for model_name, model in models.items():\n",
    "    result[model_name] = create_result(model_name, model, general_prompts, specific_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734aba7-1a1f-4146-b49a-5bb203d5bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61f157-d658-4737-b94e-3e7cdcce1ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
